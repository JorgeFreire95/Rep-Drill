# GuÃ­a de Despliegue del Chatbot IA con Ollama

## ðŸ“‹ Prerequisitos

- Docker y Docker Compose instalados
- Al menos 8GB de RAM disponible (recomendado 16GB)
- 10GB de espacio en disco para los modelos de Ollama
- Variables de entorno configuradas en `.env`

## ðŸš€ Paso 1: Configurar Variables de Entorno

Agrega las siguientes variables a tu archivo `.env`:

```env
# Ollama Configuration
OLLAMA_MODEL=llama3.2:3b
OLLAMA_TIMEOUT=60

# Chatbot Configuration
CHATBOT_MAX_HISTORY_MESSAGES=5
CHATBOT_CONTEXT_CACHE_TIMEOUT=300
CHATBOT_MAX_TOKENS=800
CHATBOT_TEMPERATURE=0.7
```

## ðŸ—ï¸ Paso 2: Construir y Levantar Servicios

```powershell
# Construir la imagen del chatbot
docker-compose build chatbot

# Levantar todos los servicios (incluye ollama y chatbot)
docker-compose up -d
```

## ðŸ“¥ Paso 3: Descargar Modelo de Ollama

Espera 30 segundos a que Ollama inicie completamente, luego descarga el modelo:

```powershell
# Modelo ligero (recomendado para desarrollo)
docker exec rep_drill_ollama ollama pull llama3.2:3b

# O modelo mÃ¡s grande (mejor calidad, requiere mÃ¡s RAM)
# docker exec rep_drill_ollama ollama pull llama3.1:8b
```

**Tiempo de descarga**: 5-15 minutos dependiendo de tu conexiÃ³n.

## ðŸ” Paso 4: Verificar Estado del Servicio

```powershell
# Ver logs del chatbot
docker logs -f rep_drill_chatbot

# Ver logs de Ollama
docker logs -f rep_drill_ollama

# Verificar que el modelo estÃ¡ disponible
docker exec rep_drill_ollama ollama list
```

DeberÃ­as ver algo como:
```
NAME              ID              SIZE      MODIFIED
llama3.2:3b       abc123def456    2.0 GB    2 minutes ago
```

## ðŸ§ª Paso 5: Probar el Servicio

### Health Check
```powershell
curl http://localhost/chatbot/api/chatbot/health/
```

Respuesta esperada:
```json
{
  "status": "ok",
  "ollama": {
    "status": "ok",
    "ollama_running": true,
    "model_available": true,
    "configured_model": "llama3.2:3b"
  },
  "analytics_service": { "status": "ok" },
  "database": { "status": "ok" },
  "redis": { "status": "ok" }
}
```

### Preguntas RÃ¡pidas
```powershell
curl http://localhost/chatbot/api/chatbot/quick-questions/
```

### Test de Pregunta (requiere JWT token)
```powershell
# Primero obtÃ©n un token de autenticaciÃ³n
$token = "tu-jwt-token-aqui"

# EnvÃ­a una pregunta
$headers = @{
    "Authorization" = "Bearer $token"
    "Content-Type" = "application/json"
}

$body = @{
    question = "Â¿CÃ³mo estÃ¡n las ventas proyectadas para esta semana?"
    periods = 30
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost/chatbot/api/chatbot/ask/" -Method POST -Headers $headers -Body $body
```

## ðŸ”§ Paso 6: Aplicar Migraciones de Base de Datos

```powershell
docker exec rep_drill_chatbot python manage.py makemigrations
docker exec rep_drill_chatbot python manage.py migrate
```

## ðŸŒ Paso 7: Probar en el Frontend

1. Abre el navegador en `http://localhost:3000` (o `http://localhost/app/`)
2. Inicia sesiÃ³n con tus credenciales
3. Ve a la pÃ¡gina **Forecasting**
4. Haz clic en el botÃ³n flotante azul con el icono de chat (esquina inferior derecha)
5. Prueba con una pregunta rÃ¡pida o escribe tu propia pregunta

## ðŸŽ¯ Preguntas de Prueba Recomendadas

- "Â¿CÃ³mo estÃ¡n las ventas proyectadas para esta semana?"
- "Â¿QuÃ© productos tienen mayor riesgo de stockout?"
- "Dame un resumen de las alertas crÃ­ticas"
- "Â¿CuÃ¡l es la tendencia de ventas del Ãºltimo mes?"

## âš¡ OptimizaciÃ³n de Performance

### Para Desarrollo (RÃ¡pido, menos preciso)
```powershell
# Usar modelo ligero
docker exec rep_drill_ollama ollama pull llama3.2:3b
```

Actualiza `.env`:
```env
OLLAMA_MODEL=llama3.2:3b
CHATBOT_MAX_TOKENS=500
```

### Para ProducciÃ³n (Mejor calidad)
```powershell
# Usar modelo mÃ¡s grande
docker exec rep_drill_ollama ollama pull llama3.1:8b
```

Actualiza `.env`:
```env
OLLAMA_MODEL=llama3.1:8b
CHATBOT_MAX_TOKENS=800
```

Reinicia el servicio:
```powershell
docker-compose restart chatbot
```

## ðŸ› Troubleshooting

### Error: "No se pudo conectar a Ollama"

**Causa**: Ollama no estÃ¡ corriendo o no ha terminado de iniciar.

**SoluciÃ³n**:
```powershell
# Verificar que Ollama estÃ¡ corriendo
docker ps | findstr ollama

# Ver logs de Ollama
docker logs rep_drill_ollama

# Reiniciar Ollama
docker-compose restart ollama

# Esperar 30 segundos y reintentar
```

### Error: "Modelo no encontrado"

**Causa**: No has descargado el modelo configurado.

**SoluciÃ³n**:
```powershell
# Listar modelos disponibles
docker exec rep_drill_ollama ollama list

# Descargar el modelo configurado
docker exec rep_drill_ollama ollama pull llama3.2:3b
```

### Error: "Timeout esperando respuesta"

**Causa**: El modelo estÃ¡ tardando demasiado (CPU lento, sin GPU).

**SoluciÃ³n 1** - Aumentar timeout:
```env
OLLAMA_TIMEOUT=120
```

**SoluciÃ³n 2** - Usar modelo mÃ¡s ligero:
```powershell
docker exec rep_drill_ollama ollama pull llama3.2:1b  # Modelo ultra ligero
```

### Error: "Analytics service no disponible"

**Causa**: El servicio de analytics no estÃ¡ corriendo.

**SoluciÃ³n**:
```powershell
# Verificar que analytics estÃ¡ corriendo
docker ps | findstr analytics

# Levantar analytics
docker-compose up -d analytics

# Verificar health
curl http://localhost:8003/health/
```

### Chatbot responde en inglÃ©s en lugar de espaÃ±ol

**Causa**: El modelo no entiende bien el prompt en espaÃ±ol.

**SoluciÃ³n**: Usa modelo con mejor soporte en espaÃ±ol:
```powershell
docker exec rep_drill_ollama ollama pull mistral:7b
```

Actualiza `.env`:
```env
OLLAMA_MODEL=mistral:7b
```

### Alto uso de RAM

**Causa**: Modelos grandes consumen mucha memoria.

**Recomendaciones**:
- `llama3.2:1b` â†’ ~1GB RAM
- `llama3.2:3b` â†’ ~3GB RAM
- `llama3.1:8b` â†’ ~6GB RAM
- `mistral:7b` â†’ ~5GB RAM

Cambia a un modelo mÃ¡s ligero si tienes RAM limitada.

## ðŸ“Š Monitoreo

### Ver mÃ©tricas del chatbot
```powershell
docker exec rep_drill_chatbot python manage.py shell

>>> from chatbot.models import ChatAnalytics
>>> from datetime import date
>>> analytics = ChatAnalytics.objects.filter(date=date.today()).first()
>>> print(f"Conversaciones: {analytics.total_conversations}")
>>> print(f"Mensajes: {analytics.total_messages}")
>>> print(f"Tokens: {analytics.total_tokens}")
>>> print(f"Tiempo promedio: {analytics.avg_response_time_ms}ms")
```

### Ver logs en tiempo real
```powershell
# Chatbot
docker logs -f rep_drill_chatbot

# Ollama
docker logs -f rep_drill_ollama
```

## ðŸ”„ ActualizaciÃ³n

Si actualizas el cÃ³digo del chatbot:

```powershell
# Rebuild
docker-compose build chatbot

# Aplicar migraciones si hay cambios en modelos
docker exec rep_drill_chatbot python manage.py migrate

# Reiniciar
docker-compose restart chatbot
```

## ðŸ“¦ Recursos del Sistema

### MÃ­nimos
- CPU: 2 cores
- RAM: 8GB
- Disco: 10GB

### Recomendados
- CPU: 4 cores
- RAM: 16GB
- Disco: 20GB
- GPU: Opcional pero mejora mucho la velocidad

### Con GPU (opcional)
Si tienes GPU NVIDIA y quieres acelerar Ollama:

1. Instala [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)

2. Actualiza `docker-compose.yml`:
```yaml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

3. Reinicia:
```powershell
docker-compose down
docker-compose up -d
```

## âœ… Checklist de Despliegue

- [ ] Variables de entorno configuradas en `.env`
- [ ] Servicios levantados: `docker-compose up -d`
- [ ] Modelo de Ollama descargado: `ollama pull llama3.2:3b`
- [ ] Migraciones aplicadas: `python manage.py migrate`
- [ ] Health check exitoso: `/chatbot/api/chatbot/health/`
- [ ] Frontend accesible: `http://localhost:3000`
- [ ] BotÃ³n de chatbot visible en pÃ¡gina Forecasting
- [ ] Primera pregunta de prueba exitosa

## ðŸŽ“ Uso del Chatbot

### Buenas PrÃ¡cticas
- **SÃ© especÃ­fico**: "Â¿CuÃ¡l es la tendencia de ventas de los Ãºltimos 7 dÃ­as?" en lugar de "Â¿CÃ³mo van las ventas?"
- **Contexto**: El chatbot recuerda los Ãºltimos 5 mensajes de la conversaciÃ³n
- **Preguntas rÃ¡pidas**: Usa los botones sugeridos para consultas comunes
- **Limpia la sesiÃ³n**: Si cambias de tema, limpia la conversaciÃ³n

### Ejemplos de Preguntas
- AnÃ¡lisis: "Â¿CuÃ¡l es la tendencia de ventas para el prÃ³ximo mes?"
- Alertas: "Â¿QuÃ© productos tienen riesgo crÃ­tico de stockout?"
- Recomendaciones: "Â¿QuÃ© deberÃ­a ordenar con prioridad?"
- MÃ©tricas: "Â¿QuÃ© tan confiable es el pronÃ³stico actual?"
- Comparaciones: "Â¿CÃ³mo se comparan las ventas de esta semana con la anterior?"

## ðŸ“š Referencias

- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [LLaMA 3.2 Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B)
- [Prophet Forecasting](https://facebook.github.io/prophet/)
- [Django REST Framework](https://www.django-rest-framework.org/)

## ðŸ†˜ Soporte

Si encuentras problemas:

1. Revisa los logs: `docker logs rep_drill_chatbot`
2. Verifica el health check: `curl http://localhost/chatbot/api/chatbot/health/`
3. Consulta la documentaciÃ³n en `backend/servicio_chatbot/README.md`
4. Abre un issue en el repositorio con logs y detalles del error
